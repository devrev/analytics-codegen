# Complex Base Query - Inputs and Errors

## 1. AWS vs OpenAI Cost Trends with Moving Averages and YoY Growth

### Input

Given the table schema:
Table name: memory.aws_daily_cost_v7
Table schema: service VARCHAR env VARCHAR region VARCHAR usage_date DATE cost DOUBLE
Table name: memory.openai_daily_costs_v3
Table schema: usage_date DATE project_name VARCHAR amount INTEGER
Generate an appropriate SQL query for a widget of AWS vs OpenAI cost trends with moving averages and YoY growth in a line graph.

### Output 1

```sql
WITH aws_costs AS (
    SELECT
        DATE_TRUNC('month', usage_date) AS month,
        SUM(cost) AS aws_cost
    FROM
        memory.aws_daily_cost_v7
    GROUP BY
        DATE_TRUNC('month', usage_date)
),

openai_costs AS (
    SELECT
        DATE_TRUNC('month', usage_date) AS month,
        SUM(amount) AS openai_cost
    FROM
        memory.openai_daily_costs_v3
    GROUP BY
        DATE_TRUNC('month', usage_date)
),

combined_costs AS (
    SELECT
        COALESCE(a.month, o.month) AS month,
        COALESCE(a.aws_cost, 0) AS aws_cost,
        COALESCE(o.openai_cost, 0) AS openai_cost
    FROM
        aws_costs a
        FULL OUTER JOIN openai_costs o ON a.month = o.month
),

with_moving_avg AS (
    SELECT
        month,
        aws_cost,
        openai_cost,
        AVG(aws_cost) OVER (
            ORDER BY month
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) AS aws_3m_moving_avg,
        AVG(openai_cost) OVER (
            ORDER BY month
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) AS openai_3m_moving_avg
    FROM
        combined_costs
),

with_yoy_growth AS (
    SELECT
        c.month,
        c.aws_cost,
        c.openai_cost,
        c.aws_3m_moving_avg,
        c.openai_3m_moving_avg,
        CASE
            WHEN py.aws_cost > 0 THEN ((c.aws_cost - py.aws_cost) / py.aws_cost) * 100
            ELSE NULL
        END AS aws_yoy_growth,
        CASE
            WHEN py.openai_cost > 0 THEN ((c.openai_cost - py.openai_cost) / py.openai_cost) * 100
            ELSE NULL
        END AS openai_yoy_growth
    FROM
        with_moving_avg c
        LEFT JOIN with_moving_avg py ON py.month = DATE_ADD('year', -1, c.month)
)

SELECT
    month,
    aws_cost,
    openai_cost,
    aws_3m_moving_avg,
    openai_3m_moving_avg,
    ROUND(aws_yoy_growth, 1) AS aws_yoy_growth_pct,
    ROUND(openai_yoy_growth, 1) AS openai_yoy_growth_pct
FROM
    with_yoy_growth
WHERE
    month >= DATE_ADD('month', -24, CURRENT_DATE)
ORDER BY
    month
```

### Error 1

Binder Error: No function matches the given name and argument types 'date_trunc(STRING_LITERAL, VARCHAR)'. You might need to add explicit type casts. Candidate functions: date_trunc(VARCHAR, TIMESTAMP) -> TIMESTAMP date_trunc(VARCHAR, DATE) -> TIMESTAMP date_trunc(VARCHAR, INTERVAL) -> INTERVAL LINE 8: DATE_TRUNC('month', usage_date) ^

### Output 2

```sql
WITH aws_costs AS (
    SELECT
        DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month,
        SUM(cost) AS aws_cost
    FROM
        memory.aws_daily_cost_v7
    GROUP BY
        DATE_TRUNC('month', CAST(usage_date AS DATE))
),

openai_costs AS (
    SELECT
        DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month,
        SUM(amount) AS openai_cost
    FROM
        memory.openai_daily_costs_v3
    GROUP BY
        DATE_TRUNC('month', CAST(usage_date AS DATE))
),

combined_costs AS (
    SELECT
        COALESCE(a.month, o.month) AS month,
        COALESCE(a.aws_cost, 0) AS aws_cost,
        COALESCE(o.openai_cost, 0) AS openai_cost
    FROM
        aws_costs a
        FULL OUTER JOIN openai_costs o ON a.month = o.month
),

with_moving_avg AS (
    SELECT
        month,
        aws_cost,
        openai_cost,
        AVG(aws_cost) OVER (
            ORDER BY month
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) AS aws_3m_moving_avg,
        AVG(openai_cost) OVER (
            ORDER BY month
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) AS openai_3m_moving_avg
    FROM
        combined_costs
),

with_yoy_growth AS (
    SELECT
        c.month,
        c.aws_cost,
        c.openai_cost,
        c.aws_3m_moving_avg,
        c.openai_3m_moving_avg,
        CASE
            WHEN py.aws_cost > 0 THEN ((c.aws_cost - py.aws_cost) / py.aws_cost) * 100
            ELSE NULL
        END AS aws_yoy_growth,
        CASE
            WHEN py.openai_cost > 0 THEN ((c.openai_cost - py.openai_cost) / py.openai_cost) * 100
            ELSE NULL
        END AS openai_yoy_growth
    FROM
        with_moving_avg c
        LEFT JOIN with_moving_avg py ON py.month = DATE_ADD('year', -1, c.month)
)

SELECT
    month,
    aws_cost,
    openai_cost,
    aws_3m_moving_avg,
    openai_3m_moving_avg,
    ROUND(aws_yoy_growth, 1) AS aws_yoy_growth_pct,
    ROUND(openai_yoy_growth, 1) AS openai_yoy_growth_pct
FROM
    with_yoy_growth
WHERE
    month >= DATE_ADD('month', -24, CURRENT_DATE)
ORDER BY
    month
```

### Error 3

Binder Error: Macro function 'date_add(date, interval)' requires 2 positional arguments, but 3 positional arguments were provided. LINE 65: ...ts AS ( SELECT DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month, SUM(cost) AS aws_cost FROM memory.aws_daily_cost_v7 GROUP BY DATE_TRUNC('month', CAST(usage_date AS DATE)) ), openai_costs AS ( SELECT DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month, SUM(amount) AS openai_cost FROM memory.openai_daily_costs_v3 GROUP BY DATE_TRUNC('month', CAST(usage_date AS DATE)) ), combined_costs AS ( SELECT COALESCE(a.month, o.month) AS month, COALESCE(a.aws_cost, 0) AS aws_cost, COALESCE(o.openai_cost, 0) AS openai_cost FROM aws_costs a FULL OUTER JOIN openai_costs o ON a.month = o.month ), with_moving_avg AS ( SELECT month, aws_cost, openai_cost, AVG(aws_cost) OVER ( ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS aws_3m_moving_avg, AVG(openai_cost) OVER ( ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS openai_3m_moving_avg FROM combined_costs ), with_yoy_growth AS ( SELECT c.month, c.aws_cost, c.openai_cost, c.aws_3m_moving_avg, c.openai_3m_moving_avg, CASE WHEN py.aws_cost > 0 THEN ((c.aws_cost - py.aws_cost) / py.aws_cost) 100 ELSE NULL END AS aws_yoy_growth, CASE WHEN py.openai_cost > 0 THEN ((c.openai_cost - py.openai_cost) / py.openai_cost) 100 ELSE NULL END AS openai_yoy_growth FROM with_moving_avg c LEFT JOIN with_moving_avg py ON py.month = DATE_ADD('year', -1, c.month) ^

### Output 3

```sql
WITH aws_costs AS (
    SELECT
        DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month,
        SUM(cost) AS aws_cost
    FROM
        memory.aws_daily_cost_v7
    GROUP BY
        DATE_TRUNC('month', CAST(usage_date AS DATE))
),

openai_costs AS (
    SELECT
        DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month,
        SUM(amount) AS openai_cost
    FROM
        memory.openai_daily_costs_v3
    GROUP BY
        DATE_TRUNC('month', CAST(usage_date AS DATE))
),

combined_costs AS (
    SELECT
        COALESCE(a.month, o.month) AS month,
        COALESCE(a.aws_cost, 0) AS aws_cost,
        COALESCE(o.openai_cost, 0) AS openai_cost
    FROM
        aws_costs a
        FULL OUTER JOIN openai_costs o ON a.month = o.month
),

with_moving_avg AS (
    SELECT
        month,
        aws_cost,
        openai_cost,
        AVG(aws_cost) OVER (
            ORDER BY month
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) AS aws_3m_moving_avg,
        AVG(openai_cost) OVER (
            ORDER BY month
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) AS openai_3m_moving_avg
    FROM
        combined_costs
),

with_yoy_growth AS (
    SELECT
        c.month,
        c.aws_cost,
        c.openai_cost,
        c.aws_3m_moving_avg,
        c.openai_3m_moving_avg,
        CASE
            WHEN py.aws_cost > 0 THEN ((c.aws_cost - py.aws_cost) / py.aws_cost) * 100
            ELSE NULL
        END AS aws_yoy_growth,
        CASE
            WHEN py.openai_cost > 0 THEN ((c.openai_cost - py.openai_cost) / py.openai_cost) * 100
            ELSE NULL
        END AS openai_yoy_growth
    FROM
        with_moving_avg c
        LEFT JOIN with_moving_avg py ON ADD_MONTHS(py.month, 12) = c.month
)

SELECT
    month,
    aws_cost,
    openai_cost,
    aws_3m_moving_avg,
    openai_3m_moving_avg,
    ROUND(aws_yoy_growth, 1) AS aws_yoy_growth_pct,
    ROUND(openai_yoy_growth, 1) AS openai_yoy_growth_pct
FROM
    with_yoy_growth
WHERE
    month >= ADD_MONTHS(CURRENT_DATE, -24)
ORDER BY
    month
```

### Error 3

Catalog Error: Scalar Function with name add*months does not exist! Did you mean "month"? LINE 65: ...ITH aws_costs AS ( SELECT DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month, SUM(cost) AS aws_cost FROM memory.aws_daily_cost_v7 GROUP BY DATE_TRUNC('month', CAST(usage_date AS DATE)) ), openai_costs AS ( SELECT DATE_TRUNC('month', CAST(usage_date AS DATE)) AS month, SUM(amount) AS openai_cost FROM memory.openai_daily_costs_v3 GROUP BY DATE_TRUNC('month', CAST(usage_date AS DATE)) ), combined_costs AS ( SELECT COALESCE(a.month, o.month) AS month, COALESCE(a.aws_cost, 0) AS aws_cost, COALESCE(o.openai_cost, 0) AS openai_cost FROM aws_costs a FULL OUTER JOIN openai_costs o ON a.month = o.month ), with_moving_avg AS ( SELECT month, aws_cost, openai_cost, AVG(aws_cost) OVER ( ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS aws_3m_moving_avg, AVG(openai_cost) OVER ( ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS openai_3m_moving_avg FROM combined_costs ), with_yoy_growth AS ( SELECT c.month, c.aws_cost, c.openai_cost, c.aws_3m_moving_avg, c.openai_3m_moving_avg, CASE WHEN py.aws_cost > 0 THEN ((c.aws_cost - py.aws_cost) / py.aws_cost) * 100 ELSE NULL END AS aws*yoy_growth, CASE WHEN py.openai_cost > 0 THEN ((c.openai_cost - py.openai_cost) / py.openai_cost) * 100 ELSE NULL END AS openai_yoy_growth FROM with_moving_avg c LEFT JOIN with_moving_avg py ON ADD_MONTHS(py.month, 12) = c.month ^

## 2. Project Cost Distribution with Variance Analysis and Anomaly Detection

### Input

Given the table schema:
Table name: memory.openai_daily_costs_v3
Table schema: usage_date DATE project_name VARCHAR amount INTEGER
Generate an appropriate SQL query for a widget of project cost distribution with variance analysis and anomaly detection in a heatmap.

### Output 1

```sql
Binder Error: Cannot compare values of type VARCHAR and type TIMESTAMP - an explicit cast is required LINE 13: WHERE usage_date >= CURRENT_DATE - INTERVAL '90 days' ^

```

### Error 1

Binder Error: Cannot compare values of type VARCHAR and type TIMESTAMP - an explicit cast is required LINE 13: WHERE usage_date >= CURRENT_DATE - INTERVAL '90 days' ^

### Output 2

```sql
WITH
-- Calculate daily costs and statistics by project
project_metrics AS (
    SELECT
        project_name,
        usage_date,
        amount AS daily_cost,
        AVG(amount) OVER (PARTITION BY project_name ORDER BY usage_date
            ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) AS avg_30d_cost,
        STDDEV(amount) OVER (PARTITION BY project_name ORDER BY usage_date
            ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) AS stddev_30d_cost
    FROM memory.openai_daily_costs_v3
    WHERE usage_date >= CAST(CURRENT_DATE - INTERVAL '90 days' AS DATE)
),

-- Calculate z-scores and anomaly flags
anomaly_detection AS (
    SELECT
        project_name,
        usage_date,
        daily_cost,
        avg_30d_cost,
        stddev_30d_cost,
        CASE
            WHEN stddev_30d_cost = 0 OR stddev_30d_cost IS NULL THEN 0
            ELSE (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0)
        END AS z_score,
        CASE
            WHEN stddev_30d_cost = 0 OR stddev_30d_cost IS NULL THEN 'normal'
            WHEN (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0) > 2 THEN 'high_anomaly'
            WHEN (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0) < -2 THEN 'low_anomaly'
            ELSE 'normal'
        END AS anomaly_flag
    FROM project_metrics
),

-- Calculate project statistics for the heatmap
project_heatmap_data AS (
    SELECT
        project_name,
        usage_date,
        daily_cost,
        z_score,
        anomaly_flag,
        -- Calculate relative cost compared to project average
        daily_cost / NULLIF(AVG(daily_cost) OVER (PARTITION BY project_name), 0) AS relative_cost,
        -- Calculate cost percentage of total for that day
        daily_cost / NULLIF(SUM(daily_cost) OVER (PARTITION BY usage_date), 0) * 100 AS pct_of_daily_total,
        -- Week over week change
        daily_cost - LAG(daily_cost, 7) OVER (PARTITION BY project_name ORDER BY usage_date) AS wow_change,
        -- Month over month change
        daily_cost - LAG(daily_cost, 30) OVER (PARTITION BY project_name ORDER BY usage_date) AS mom_change
    FROM anomaly_detection
)

-- Final query for the heatmap visualization
SELECT
    project_name,
    usage_date,
    daily_cost,
    z_score,
    anomaly_flag,
    relative_cost,
    pct_of_daily_total,
    wow_change,
    CASE
        WHEN wow_change > 0 THEN 'increase'
        WHEN wow_change < 0 THEN 'decrease'
        ELSE 'stable'
    END AS wow_trend,
    mom_change,
    CASE
        WHEN mom_change > 0 THEN 'increase'
        WHEN mom_change < 0 THEN 'decrease'
        ELSE 'stable'
    END AS mom_trend,
    -- Calculate variance metrics for color intensity in heatmap
    CASE
        WHEN anomaly_flag = 'high_anomaly' THEN 2
        WHEN anomaly_flag = 'low_anomaly' THEN -2
        WHEN z_score > 1 THEN 1
        WHEN z_score < -1 THEN -1
        ELSE 0
    END AS heatmap_intensity
FROM project_heatmap_data
WHERE usage_date >= CAST(CURRENT_DATE - INTERVAL '30 days' AS DATE)
ORDER BY usage_date, pct_of_daily_total DESC
```

### Error 2

Binder Error: Cannot compare values of type VARCHAR and type DATE - an explicit cast is required LINE 13: WHERE usage_date >= CAST(CURRENT_DATE - INTERVAL '90 days' AS DATE) ), -- Calculate z-scores and anomaly flags anomaly_detection AS ( SELECT project_name, usage_date, daily_cost, avg_30d_cost, stddev_30d_cost, CASE WHEN stddev_30d_cost = 0 OR stddev_30d_cost IS NULL THEN 0 ELSE (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0) END AS z_score, CASE WHEN stddev_30d_cost = 0 OR stddev_30d_cost IS NULL THEN 'normal' WHEN (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0) > 2 THEN 'high_anomaly' WHEN (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0) < -2 THEN 'low_anomaly' ELSE 'normal' END AS anomaly_flag FROM project_metrics ), -- Calculate project statistics for the heatmap project_heatmap_data AS ( SELECT project_name, usage_date, daily_cost, z_score, anomaly_flag, -- Calculate relative cost compared to project average daily_cost / NULLIF(AVG(daily_cost) OVER (PARTITION BY project_name), 0) AS relative_cost, -- Calculate cost percentage of total for that day daily_cost / NULLIF(SUM(daily_cost) OVER (PARTITION BY usage_date), 0) \* 100 AS pct_of_daily_total, -- Week over week change daily_cost - LAG(daily_cost, 7) OVER (PARTITION BY project_name ORDER BY usage_date) AS wow_change, -- Month over month change daily_cost - LAG(daily_cost, 30) OVER (PARTITION BY project_name ORDER BY usage_date) AS mom_change FROM anomaly_detection ) -- Final query for the heatmap visualization SELECT project_name, usage_date, daily_cost, z_score, anomaly_flag, relative_cost, pct_of_daily_total, wow_change, CASE WHEN wow_change > 0 THEN 'increase' WHEN wow_change < 0 THEN 'decrease' ELSE 'stable' END AS wow_trend, mom_change, CASE WHEN mom_change > 0 THEN 'increase' WHEN mom_change < 0 THEN 'decrease' ELSE 'stable' END AS mom_trend, -- Calculate variance metrics for color intensity in heatmap CASE WHEN anomaly_flag = 'high_anomaly' THEN 2 WHEN anomaly_flag = 'low_anomaly' THEN -2 WHEN z_score > 1 THEN 1 WHEN z_score < -1 THEN -1 ELSE 0 END AS heatmap_intensity FROM project_heatmap_data WHERE usage_date >= CAST(CURRENT_DATE - INTERVAL '30 days' AS DATE) ORDER BY usage_date, pct_of_daily_total DESC... ^

### Output 3: SUCCESS

```sql
WITH
-- Calculate daily costs and statistics by project
project_metrics AS (
    SELECT
        project_name,
        CAST(usage_date AS DATE) AS usage_date,
        amount AS daily_cost,
        AVG(amount) OVER (PARTITION BY project_name ORDER BY CAST(usage_date AS DATE)
            ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) AS avg_30d_cost,
        STDDEV(amount) OVER (PARTITION BY project_name ORDER BY CAST(usage_date AS DATE)
            ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) AS stddev_30d_cost
    FROM memory.openai_daily_costs_v3
    WHERE CAST(usage_date AS DATE) >= CURRENT_DATE - INTERVAL '90 days'
),

-- Calculate z-scores and anomaly flags
anomaly_detection AS (
    SELECT
        project_name,
        usage_date,
        daily_cost,
        avg_30d_cost,
        stddev_30d_cost,
        CASE
            WHEN stddev_30d_cost = 0 OR stddev_30d_cost IS NULL THEN 0
            ELSE (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0)
        END AS z_score,
        CASE
            WHEN stddev_30d_cost = 0 OR stddev_30d_cost IS NULL THEN 'normal'
            WHEN (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0) > 2 THEN 'high_anomaly'
            WHEN (daily_cost - avg_30d_cost) / NULLIF(stddev_30d_cost, 0) < -2 THEN 'low_anomaly'
            ELSE 'normal'
        END AS anomaly_flag
    FROM project_metrics
),

-- Calculate project statistics for the heatmap
project_heatmap_data AS (
    SELECT
        project_name,
        usage_date,
        daily_cost,
        z_score,
        anomaly_flag,
        -- Calculate relative cost compared to project average
        daily_cost / NULLIF(AVG(daily_cost) OVER (PARTITION BY project_name), 0) AS relative_cost,
        -- Calculate cost percentage of total for that day
        daily_cost / NULLIF(SUM(daily_cost) OVER (PARTITION BY usage_date), 0) * 100 AS pct_of_daily_total,
        -- Week over week change
        daily_cost - LAG(daily_cost, 7) OVER (PARTITION BY project_name ORDER BY usage_date) AS wow_change,
        -- Month over month change
        daily_cost - LAG(daily_cost, 30) OVER (PARTITION BY project_name ORDER BY usage_date) AS mom_change
    FROM anomaly_detection
)

-- Final query for the heatmap visualization
SELECT
    project_name,
    usage_date,
    daily_cost,
    z_score,
    anomaly_flag,
    relative_cost,
    pct_of_daily_total,
    wow_change,
    CASE
        WHEN wow_change > 0 THEN 'increase'
        WHEN wow_change < 0 THEN 'decrease'
        ELSE 'stable'
    END AS wow_trend,
    mom_change,
    CASE
        WHEN mom_change > 0 THEN 'increase'
        WHEN mom_change < 0 THEN 'decrease'
        ELSE 'stable'
    END AS mom_trend,
    -- Calculate variance metrics for color intensity in heatmap
    CASE
        WHEN anomaly_flag = 'high_anomaly' THEN 2
        WHEN anomaly_flag = 'low_anomaly' THEN -2
        WHEN z_score > 1 THEN 1
        WHEN z_score < -1 THEN -1
        ELSE 0
    END AS heatmap_intensity
FROM project_heatmap_data
WHERE usage_date >= CURRENT_DATE - INTERVAL '30 days'
ORDER BY usage_date, pct_of_daily_total DESC
```

## 3. Communication Network Analysis with Response Rates and Thread Depth

### Input

Given the table schema:
Table name: memory.email_and_linkedin_chats
Table schema: id VARCHAR created_date TIMESTAMP subject VARCHAR sender_address VARCHAR sender_user_id VARCHAR receiver_address VARCHAR[] receiver_user_id VARCHAR[] cc_address VARCHAR[] cc_user_id VARCHAR[]
Generate an appropriate SQL query for a widget of communication network analysis with response rates and thread depth in a pack bubble.

### Output: SUCCESS

```sql
WITH thread_analysis AS (
  SELECT
    sender_user_id,
    receiver_user_id_unnested,
    COUNT(DISTINCT id) AS message_count,
    AVG(response_time_hours) AS avg_response_time,
    AVG(thread_depth) AS avg_thread_depth
  FROM (
    SELECT
      e1.id,
      e1.sender_user_id,
      UNNEST(e1.receiver_user_id) AS receiver_user_id_unnested,
      -- Calculate response time (in hours) between messages in the same thread
      EXTRACT(EPOCH FROM (
        LEAD(e1.created_date) OVER (PARTITION BY e1.subject ORDER BY e1.created_date) - e1.created_date
      )) / 3600 AS response_time_hours,
      -- Calculate thread depth by counting messages with the same subject
      COUNT(e1.id) OVER (PARTITION BY e1.subject) AS thread_depth
    FROM
      memory.email_and_linkedin_chats e1
    WHERE
      e1.created_date >= CURRENT_DATE - INTERVAL '90 days'
  ) AS expanded
  WHERE
    receiver_user_id_unnested IS NOT NULL
    AND sender_user_id IS NOT NULL
  GROUP BY
    sender_user_id,
    receiver_user_id_unnested
),

response_metrics AS (
  SELECT
    sender_user_id,
    receiver_user_id_unnested,
    message_count,
    avg_response_time,
    avg_thread_depth,
    -- Calculate normalized metrics for bubble size
    message_count / SUM(message_count) OVER() * 100 AS normalized_volume,
    CASE
      WHEN avg_response_time IS NULL THEN 0
      WHEN avg_response_time <= 1 THEN 1.0  -- Fast response (within an hour)
      WHEN avg_response_time <= 24 THEN 0.7 -- Same day response
      WHEN avg_response_time <= 72 THEN 0.4 -- Response within 3 days
      ELSE 0.1                              -- Slow response
    END AS response_rate_score
  FROM thread_analysis
)

SELECT
  sender_user_id AS source,
  receiver_user_id_unnested AS target,
  message_count AS communication_volume,
  ROUND(avg_response_time, 2) AS avg_response_time_hours,
  ROUND(avg_thread_depth, 1) AS avg_thread_depth,
  ROUND(normalized_volume, 2) AS normalized_volume,
  ROUND(response_rate_score, 2) AS response_rate_score,
  -- Bubble size calculation based on volume and response metrics
  ROUND(normalized_volume * (0.3 + 0.7 * response_rate_score), 2) AS bubble_size,
  -- Color intensity based on thread depth
  CASE
    WHEN avg_thread_depth >= 5 THEN 'high_engagement'
    WHEN avg_thread_depth >= 3 THEN 'medium_engagement'
    ELSE 'low_engagement'
  END AS engagement_level
FROM
  response_metrics
WHERE
  message_count >= 3  -- Filter for meaningful connections only
ORDER BY
  bubble_size DESC
LIMIT 100;  -- Limit to top 100 connections for visualization
```

## 4. User Engagement Patterns with Cohort Analysis and Retention Curves

### Input

Given the table schema:
Table name: system.user_sessions_track_events_summary_v2
Table schema: rev_oid VARCHAR total_user_sessions BIGINT total_session_length DOUBLE is_verified BOOLEAN total_events BIGINT created_at TIMESTAMP
Generate an appropriate SQL query for a widget of user engagement patterns with cohort analysis and retention curves in a table.

### Output: SUCCESS

```sql
WITH cohorts AS (
    -- Define cohorts based on user's first appearance in the dataset
    SELECT
        rev_oid,
        DATE_TRUNC('month', MIN(created_at)) AS cohort_month,
        is_verified
    FROM system.user_sessions_track_events_summary_v2
    GROUP BY rev_oid, is_verified
),

monthly_activity AS (
    -- Get user activity by month
    SELECT
        s.rev_oid,
        DATE_TRUNC('month', s.created_at) AS activity_month,
        SUM(s.total_user_sessions) AS sessions,
        SUM(s.total_session_length) AS total_duration,
        SUM(s.total_events) AS events
    FROM system.user_sessions_track_events_summary_v2 s
    GROUP BY s.rev_oid, DATE_TRUNC('month', s.created_at)
),

retention_data AS (
    -- Calculate months since joining for retention analysis
    SELECT
        c.cohort_month,
        c.is_verified,
        ma.activity_month,
        DATEDIFF('month', c.cohort_month, ma.activity_month) AS months_since_joining,
        COUNT(DISTINCT c.rev_oid) AS users,
        SUM(ma.sessions) AS total_sessions,
        SUM(ma.total_duration) AS total_duration,
        SUM(ma.events) AS total_events,
        AVG(ma.sessions) AS avg_sessions_per_user,
        AVG(ma.total_duration) AS avg_duration_per_user,
        AVG(ma.events) AS avg_events_per_user
    FROM cohorts c
    JOIN monthly_activity ma ON c.rev_oid = ma.rev_oid
    WHERE ma.activity_month >= c.cohort_month
    GROUP BY c.cohort_month, c.is_verified, ma.activity_month, months_since_joining
),

cohort_size AS (
    -- Get the initial size of each cohort for percentage calculations
    SELECT
        cohort_month,
        is_verified,
        COUNT(DISTINCT rev_oid) AS initial_users
    FROM cohorts
    GROUP BY cohort_month, is_verified
)

-- Final retention table with cohort analysis
SELECT
    rd.cohort_month,
    CASE WHEN rd.is_verified THEN 'Verified' ELSE 'Unverified' END AS user_type,
    rd.months_since_joining,
    rd.activity_month,
    rd.users,
    cs.initial_users,
    ROUND((rd.users * 100.0 / cs.initial_users), 2) AS retention_percentage,
    rd.total_sessions,
    rd.total_duration,
    rd.total_events,
    rd.avg_sessions_per_user,
    rd.avg_duration_per_user,
    rd.avg_events_per_user,
    ROUND((rd.avg_events_per_user /
          NULLIF(LAG(rd.avg_events_per_user) OVER (
              PARTITION BY rd.cohort_month, rd.is_verified
              ORDER BY rd.months_since_joining
          ), 0) - 1) * 100, 2) AS pct_change_events_from_previous
FROM retention_data rd
JOIN cohort_size cs ON rd.cohort_month = cs.cohort_month AND rd.is_verified = cs.is_verified
ORDER BY rd.cohort_month, rd.is_verified, rd.months_since_joining;
```

## 5. Cost Optimization Opportunities with Anomaly Detection and Efficiency Scoring

### Input

Given the table schema:
Table name: memory.aws_daily_cost_v7
Table schema: service VARCHAR env VARCHAR region VARCHAR usage_date DATE cost DOUBLE
Generate an appropriate SQL query for a widget of cost optimization opportunities with anomaly detection and efficiency scoring in a column chart.

### Output

```sql
WITH
-- Calculate daily cost averages per service/env/region
daily_averages AS (
    SELECT
        service,
        env,
        region,
        AVG(cost) as avg_daily_cost
    FROM
        memory.aws_daily_cost_v7
    WHERE
        usage_date >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)
    GROUP BY
        service, env, region
),

-- Calculate standard deviation for anomaly detection
cost_stats AS (
    SELECT
        service,
        env,
        region,
        AVG(cost) as avg_cost,
        STDDEV(cost) as cost_stddev
    FROM
        memory.aws_daily_cost_v7
    WHERE
        usage_date >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)
    GROUP BY
        service, env, region
),

-- Find recent costs with their metrics
recent_costs AS (
    SELECT
        c.service,
        c.env,
        c.region,
        SUM(c.cost) as recent_cost,
        cs.avg_cost,
        cs.cost_stddev,
        -- Anomaly score: how many standard deviations from mean
        (SUM(c.cost) - cs.avg_cost) / NULLIF(cs.cost_stddev, 0) as anomaly_score,
        -- Efficiency score (inverse of anomaly - positive is good)
        5 - LEAST(5, GREATEST(0, (SUM(c.cost) - cs.avg_cost) / NULLIF(cs.cost_stddev, 0))) as efficiency_score
    FROM
        memory.aws_daily_cost_v7 c
    JOIN
        cost_stats cs ON c.service = cs.service AND c.env = cs.env AND c.region = cs.region
    WHERE
        c.usage_date >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY)
    GROUP BY
        c.service, c.env, c.region, cs.avg_cost, cs.cost_stddev
)

-- Final query to surface optimization opportunities
SELECT
    service,
    env,
    region,
    recent_cost,
    avg_cost,
    ROUND((recent_cost - avg_cost) / avg_cost * 100, 1) as percent_change,
    ROUND(anomaly_score, 2) as anomaly_score,
    ROUND(efficiency_score, 1) as efficiency_score,
    CASE
        WHEN anomaly_score > 2 THEN 'Critical'
        WHEN anomaly_score > 1 THEN 'Warning'
        WHEN anomaly_score < -1 THEN 'Efficient'
        ELSE 'Normal'
    END as status
FROM
    recent_costs
WHERE
    anomaly_score > 1 OR anomaly_score < -1
ORDER BY
    anomaly_score DESC
LIMIT 15;
```

### Error

Binder Error: No function matches the given name and argument types 'date_sub(DATE, INTERVAL)'. You might need to add explicit type casts. Candidate functions: date_sub(VARCHAR, DATE, DATE) -> BIGINT date_sub(VARCHAR, TIMESTAMP, TIMESTAMP) -> BIGINT date_sub(VARCHAR, TIME, TIME) -> BIGINT LINE 28: usage_date >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY) ^

## 6. Incident Lifecycle Analysis with Resolution Efficiency and SLA Compliance

### Input: SUCCESS

Given the table schema:
Table name: system.dim_incident
Table schema: object_type VARCHAR stock_schema_fragment VARCHAR playbook_ids VARCHAR[] subtype VARCHAR stock_schema_fragment_id VARCHAR mitigated_date TIMESTAMP actual_close_date TIMESTAMP severity_json VARCHAR id VARCHAR state VARCHAR resolved_at TIMESTAMP custom_fields VARCHAR pia_ids VARCHAR[] stakeholders VARCHAR[] stage_json VARCHAR operation VARCHAR modified_date TIMESTAMP display_id VARCHAR applies_to_part_ids VARCHAR[] artifact_ids VARCHAR[] impact VARCHAR acknowledged_date TIMESTAMP identified_at TIMESTAMP source VARCHAR modified_by_id VARCHAR body VARCHAR object_version BIGINT tags_json VARCHAR created_by_id VARCHAR custom_schema_fragments VARCHAR[] owned_by_ids VARCHAR[] identified_date TIMESTAMP is_deleted BOOLEAN title VARCHAR last_system_modification_version BIGINT impacted_customers VARCHAR[] target_close_date TIMESTAMP created_date TIMESTAMP reported_by VARCHAR custom_schema_fragment_ids VARCHAR[] related_doc_ids VARCHAR[]
Generate an appropriate SQL query for a widget of incident lifecycle analysis with resolution efficiency and SLA compliance in a bar graph.

### Output

```sql
WITH incident_metrics AS (
    SELECT
        id,
        title,
        display_id,
        severity_json,
        state,
        identified_at,
        acknowledged_date,
        resolved_at,
        target_close_date,
        actual_close_date,
        -- Calculate time to acknowledge (in minutes)
        EXTRACT(EPOCH FROM (acknowledged_date - identified_at)) / 60 AS time_to_acknowledge_min,
        -- Calculate time to resolve (in minutes)
        EXTRACT(EPOCH FROM (resolved_at - identified_at)) / 60 AS time_to_resolve_min,
        -- Calculate if resolved within SLA (assuming target_close_date is the SLA deadline)
        CASE
            WHEN resolved_at <= target_close_date THEN TRUE
            WHEN resolved_at IS NULL AND CURRENT_TIMESTAMP <= target_close_date THEN TRUE
            ELSE FALSE
        END AS within_sla,
        -- Extract severity level from severity_json
        COALESCE(
            JSON_EXTRACT_PATH_TEXT(CAST(severity_json AS JSON), 'value'),
            JSON_EXTRACT_PATH_TEXT(CAST(severity_json AS JSON), 'label'),
            'Unknown'
        ) AS severity_level
    FROM
        system.dim_incident
    WHERE
        is_deleted = FALSE
        AND identified_at IS NOT NULL
        -- Limit to incidents created in the last 90 days or customize as needed
        AND created_date >= CURRENT_DATE - INTERVAL '90 days'
)

SELECT
    severity_level,
    COUNT(*) AS total_incidents,
    ROUND(AVG(time_to_acknowledge_min), 2) AS avg_time_to_acknowledge_min,
    ROUND(AVG(time_to_resolve_min), 2) AS avg_time_to_resolve_min,
    ROUND((SUM(CASE WHEN within_sla THEN 1 ELSE 0 END)::FLOAT / COUNT(*)) * 100, 2) AS sla_compliance_percentage,
    -- Metrics for resolution efficiency
    ROUND(AVG(CASE WHEN state = 'RESOLVED' THEN time_to_resolve_min END), 2) AS avg_resolution_time_min,
    COUNT(CASE WHEN state = 'RESOLVED' THEN 1 END) AS resolved_incidents,
    COUNT(CASE WHEN state != 'RESOLVED' THEN 1 END) AS open_incidents
FROM
    incident_metrics
GROUP BY
    severity_level
ORDER BY
    -- Order by severity (assuming severity levels might be like 'High', 'Medium', 'Low')
    CASE
        WHEN severity_level = 'Critical' THEN 1
        WHEN severity_level = 'High' THEN 2
        WHEN severity_level = 'Medium' THEN 3
        WHEN severity_level = 'Low' THEN 4
        ELSE 5
    END;
```

## 7. Content Engagement Analytics with Retention Metrics and Author Performance

### Input

Given the table schema:
Table name: system.article_views_and_votes_summary
Table schema: article_id VARCHAR record_date DATE account_id VARCHAR surface VARCHAR is_verified BOOLEAN total_views BIGINT total_duration_ms BIGINT unique_views BIGINT article_upvotes BIGINT article_downvotes BIGINT
Table name: system.dim_article
Table schema: stock_schema_fragment_id VARCHAR object_version BIGINT last_system_modification_version BIGINT modified_date TIMESTAMP authored_by_ids VARCHAR[] sync_metadata VARCHAR created_date TIMESTAMP owned_by_ids VARCHAR[] shared_with VARCHAR extracted_content VARCHAR[] external_source_data VARCHAR created_by_id VARCHAR modified_by_id VARCHAR subtype VARCHAR resource_json VARCHAR object_type VARCHAR operation VARCHAR tags_json VARCHAR custom_schema_fragment_ids VARCHAR[] status VARCHAR num_downvotes BIGINT title VARCHAR rank VARCHAR custom_fields VARCHAR display_id VARCHAR scope VARCHAR access_level BIGINT num_upvotes BIGINT applies_to_part_ids VARCHAR[] aliases VARCHAR[] id VARCHAR is_deleted BOOLEAN parent VARCHAR description VARCHAR type BIGINT published_date TIMESTAMP language VARCHAR
Generate an appropriate SQL query for a widget of content engagement analytics with retention metrics and author performance correlation in a donut chart.

### Output: SUCCESS

```sql
WITH article_engagement_metrics AS (
    SELECT
        a.id AS article_id,
        a.title,
        a.authored_by_ids[1] AS primary_author_id,
        AVG(avs.total_duration_ms::FLOAT / NULLIF(avs.total_views, 0)) AS avg_read_time_ms,
        SUM(avs.total_views) AS total_views,
        SUM(avs.unique_views) AS unique_views,
        SUM(avs.article_upvotes) AS total_upvotes,
        SUM(avs.article_downvotes) AS total_downvotes,
        CASE
            WHEN SUM(avs.article_upvotes + avs.article_downvotes) = 0 THEN 0
            ELSE ROUND(SUM(avs.article_upvotes) * 100.0 / SUM(avs.article_upvotes + avs.article_downvotes), 2)
        END AS positive_engagement_rate
    FROM
        system.article_views_and_votes_summary avs
    JOIN
        system.dim_article a ON avs.article_id = a.id
    WHERE
        a.is_deleted = FALSE
        AND avs.record_date >= CURRENT_DATE - INTERVAL '90 days'
    GROUP BY
        a.id, a.title, a.authored_by_ids[1]
),
author_performance AS (
    SELECT
        primary_author_id,
        COUNT(DISTINCT article_id) AS articles_published,
        SUM(total_views) AS author_total_views,
        SUM(unique_views) AS author_unique_views,
        ROUND(AVG(positive_engagement_rate), 2) AS avg_positive_engagement,
        SUM(total_upvotes) AS author_total_upvotes
    FROM
        article_engagement_metrics
    GROUP BY
        primary_author_id
),
engagement_categories AS (
    SELECT
        CASE
            WHEN positive_engagement_rate >= 90 THEN 'Excellent (90-100%)'
            WHEN positive_engagement_rate >= 75 THEN 'Good (75-89%)'
            WHEN positive_engagement_rate >= 50 THEN 'Average (50-74%)'
            WHEN positive_engagement_rate >= 25 THEN 'Poor (25-49%)'
            ELSE 'Very Poor (<25%)'
        END AS engagement_category,
        COUNT(*) AS article_count,
        SUM(total_views) AS category_views,
        ROUND(AVG(avg_read_time_ms), 2) AS category_avg_read_time
    FROM
        article_engagement_metrics
    GROUP BY
        CASE
            WHEN positive_engagement_rate >= 90 THEN 'Excellent (90-100%)'
            WHEN positive_engagement_rate >= 75 THEN 'Good (75-89%)'
            WHEN positive_engagement_rate >= 50 THEN 'Average (50-74%)'
            WHEN positive_engagement_rate >= 25 THEN 'Poor (25-49%)'
            ELSE 'Very Poor (<25%)'
        END
)

SELECT
    engagement_category,
    article_count,
    category_views,
    category_avg_read_time,
    ROUND(100.0 * article_count / SUM(article_count) OVER(), 2) AS percentage_of_articles
FROM
    engagement_categories
ORDER BY
    CASE engagement_category
        WHEN 'Excellent (90-100%)' THEN 1
        WHEN 'Good (75-89%)' THEN 2
        WHEN 'Average (50-74%)' THEN 3
        WHEN 'Poor (25-49%)' THEN 4
        WHEN 'Very Poor (<25%)' THEN 5
    END
```

## 8. Reader Behavior Segmentation with View-to-Vote Conversion Rates Across Surfaces

### Input

Given the table schema:
Table name: system.article_views_and_votes_summary
Table schema: article_id VARCHAR record_date DATE account_id VARCHAR surface VARCHAR is_verified BOOLEAN total_views BIGINT total_duration_ms BIGINT unique_views BIGINT article_upvotes BIGINT article_downvotes BIGINT
Table name: system.dim_article
Table schema: stock_schema_fragment_id VARCHAR object_version BIGINT last_system_modification_version BIGINT modified_date TIMESTAMP authored_by_ids VARCHAR[] sync_metadata VARCHAR created_date TIMESTAMP owned_by_ids VARCHAR[] shared_with VARCHAR extracted_content VARCHAR[] external_source_data VARCHAR created_by_id VARCHAR modified_by_id VARCHAR subtype VARCHAR resource_json VARCHAR object_type VARCHAR operation VARCHAR tags_json VARCHAR custom_schema_fragment_ids VARCHAR[] status VARCHAR num_downvotes BIGINT title VARCHAR rank VARCHAR custom_fields VARCHAR display_id VARCHAR scope VARCHAR access_level BIGINT num_upvotes BIGINT applies_to_part_ids VARCHAR[] aliases VARCHAR[] id VARCHAR is_deleted BOOLEAN parent VARCHAR description VARCHAR type BIGINT published_date TIMESTAMP language VARCHAR
Generate an appropriate SQL query for a widget of reader behavior segmentation with view-to-vote conversion rates across surfaces in a table.

### Output: SUCCESS

```sql
WITH article_metrics AS (
  SELECT
    av.surface,
    av.is_verified,
    COUNT(DISTINCT av.article_id) AS articles_count,
    SUM(av.unique_views) AS total_unique_views,
    SUM(av.article_upvotes + av.article_downvotes) AS total_votes,
    SUM(av.article_upvotes) AS total_upvotes,
    SUM(av.article_downvotes) AS total_downvotes
  FROM
    system.article_views_and_votes_summary av
  JOIN
    system.dim_article a ON av.article_id = a.id
  WHERE
    a.is_deleted = FALSE
  GROUP BY
    av.surface, av.is_verified
)

SELECT
  surface,
  CASE WHEN is_verified THEN 'Verified' ELSE 'Unverified' END AS user_type,
  articles_count,
  total_unique_views,
  total_votes,
  total_upvotes,
  total_downvotes,
  ROUND((total_votes::DECIMAL / NULLIF(total_unique_views, 0)) * 100, 2) AS view_to_vote_conversion_percentage,
  ROUND((total_upvotes::DECIMAL / NULLIF(total_votes, 0)) * 100, 2) AS upvote_percentage
FROM
  article_metrics
ORDER BY
  surface, is_verified;
```

## 9. Content Engagement Trajectories Comparing Verified vs. Unverified Readers

### Input

Given the table schema:
Table name: system.article_views_and_votes_summary
Table schema: article_id VARCHAR record_date DATE account_id VARCHAR surface VARCHAR is_verified BOOLEAN total_views BIGINT total_duration_ms BIGINT unique_views BIGINT article_upvotes BIGINT article_downvotes BIGINT
Table name: system.dim_article
Table schema: stock_schema_fragment_id VARCHAR object_version BIGINT last_system_modification_version BIGINT modified_date TIMESTAMP authored_by_ids VARCHAR[] sync_metadata VARCHAR created_date TIMESTAMP owned_by_ids VARCHAR[] shared_with VARCHAR extracted_content VARCHAR[] external_source_data VARCHAR created_by_id VARCHAR modified_by_id VARCHAR subtype VARCHAR resource_json VARCHAR object_type VARCHAR operation VARCHAR tags_json VARCHAR custom_schema_fragment_ids VARCHAR[] status VARCHAR num_downvotes BIGINT title VARCHAR rank VARCHAR custom_fields VARCHAR display_id VARCHAR scope VARCHAR access_level BIGINT num_upvotes BIGINT applies_to_part_ids VARCHAR[] aliases VARCHAR[] id VARCHAR is_deleted BOOLEAN parent VARCHAR description VARCHAR type BIGINT published_date TIMESTAMP language VARCHAR
Generate an appropriate SQL query for a widget of content engagement trajectories comparing verified vs. unverified readers in a line graph.

### Output: SUCCESS

```sql
WITH daily_metrics AS (
  SELECT
    DATE_TRUNC('day', avs.record_date) AS date,
    avs.is_verified,
    COUNT(DISTINCT avs.article_id) AS articles_viewed,
    SUM(avs.total_views) AS total_views,
    SUM(avs.unique_views) AS unique_views,
    ROUND(AVG(avs.total_duration_ms)/1000.0, 2) AS avg_view_duration_seconds,
    SUM(avs.article_upvotes) AS total_upvotes,
    SUM(avs.article_downvotes) AS total_downvotes,
    CASE WHEN SUM(avs.total_views) > 0
         THEN ROUND((SUM(avs.article_upvotes)::FLOAT / SUM(avs.total_views)) * 100, 2)
         ELSE 0 END AS upvote_rate
  FROM
    system.article_views_and_votes_summary avs
  JOIN
    system.dim_article a ON avs.article_id = a.id
  WHERE
    avs.record_date >= CURRENT_DATE - INTERVAL '30 days'
    AND a.is_deleted = FALSE
    AND a.status = 'published'
  GROUP BY
    DATE_TRUNC('day', avs.record_date),
    avs.is_verified
)

SELECT
  date,
  is_verified,
  articles_viewed,
  total_views,
  unique_views,
  avg_view_duration_seconds,
  total_upvotes,
  total_downvotes,
  upvote_rate
FROM
  daily_metrics
ORDER BY
  date ASC,
  is_verified
```

## 10. Mobile Performance Metrics by Device and Network with Error Correlation

### Input

Given the table schema:
Table name: system.mobile_sessions_metadatas_v1
Table schema: session_id VARCHAR created_at TIMESTAMP version_key VARCHAR identifier VARCHAR network_operator VARCHAR network_type VARCHAR first_user_interaction BIGINT app_launch_time BIGINT app_launch_type VARCHAR device_manufacturer VARCHAR device_name VARCHAR app_version VARCHAR exception_type VARCHAR error_count BIGINT is_rage BIGINT ux_evaluation VARCHAR platform VARCHAR
Generate an appropriate SQL query for a widget of mobile performance metrics by device and network with error correlation in a pie chart.

### Output 1

```sql
SELECT
    CONCAT(device_manufacturer, ' ', device_name) AS device,
    network_type,
    COUNT(*) AS session_count,
    SUM(error_count) AS total_errors,
    AVG(error_count) AS avg_errors_per_session,
    SUM(CASE WHEN is_rage = 1 THEN 1 ELSE 0 END) AS rage_sessions,
    AVG(first_user_interaction - app_launch_time) AS avg_time_to_interaction_ms
FROM
    system.mobile_sessions_metadatas_v1
WHERE
    created_at >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
    AND error_count > 0
GROUP BY
    device_manufacturer,
    device_name,
    network_type
ORDER BY
    total_errors DESC
LIMIT 10;
```

### Error 1

Parser Error: syntax error at or near "=" LINE 1: =SELECT ^

### Output 2

```sql
SELECT
    CONCAT(device_manufacturer, ' ', device_name) AS device,
    network_type,
    COUNT(*) AS session_count,
    SUM(error_count) AS total_errors,
    AVG(error_count) AS avg_errors_per_session,
    SUM(CASE WHEN is_rage = 1 THEN 1 ELSE 0 END) AS rage_sessions,
    AVG(first_user_interaction - app_launch_time) AS avg_time_to_interaction_ms
FROM
    system.mobile_sessions_metadatas_v1
WHERE
    created_at >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
    AND error_count > 0
GROUP BY
    device_manufacturer,
    device_name,
    network_type
ORDER BY
    total_errors DESC
LIMIT 10;
```

### Error 2

Binder Error: No function matches the given name and argument types 'date_sub(DATE, INTERVAL)'. You might need to add explicit type casts. Candidate functions: date_sub(VARCHAR, DATE, DATE) -> BIGINT date_sub(VARCHAR, TIMESTAMP, TIMESTAMP) -> BIGINT date_sub(VARCHAR, TIME, TIME) -> BIGINT LINE 12: created_at >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY) AND error_count > 0 GROUP BY device_manufacturer, device_name, network_type ORDER BY total_errors DESC LIMIT 10;... ^

### Output 3: SUCCESS

```sql
SELECT
    CONCAT(device_manufacturer, ' ', device_name) AS device,
    network_type,
    COUNT(*) AS session_count,
    SUM(error_count) AS total_errors,
    AVG(error_count) AS avg_errors_per_session,
    SUM(CASE WHEN is_rage = 1 THEN 1 ELSE 0 END) AS rage_sessions,
    AVG(first_user_interaction - app_launch_time) AS avg_time_to_interaction_ms
FROM
    system.mobile_sessions_metadatas_v1
WHERE
    created_at >= CURRENT_DATE() - INTERVAL '30' DAY
    AND error_count > 0
GROUP BY
    device_manufacturer,
    device_name,
    network_type
ORDER BY
    total_errors DESC
LIMIT 10;
```
